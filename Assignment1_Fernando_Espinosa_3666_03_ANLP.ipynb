{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fernandoespinosa/3666-applied-natural-language-processing/blob/master/SCS_3666_018_FernandoEspinosa_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q904zJ8HdV4c"
      },
      "source": [
        "\n",
        "\n",
        "##In this assignment, you will use a predefined list of Positive and negative words which is been published by Minqing Hu and Bing Liu in their paper named \"Mining and Summarizing Customer Reviews.\"\n",
        "\n",
        "##The word list is available for download from http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\n",
        "\n",
        "\n",
        "##References:\n",
        "Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, Washington, USA\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKA_nAU0XT0W",
        "outputId": "07490734-0f10-48dd-e5ca-d5b18006c940",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Setup your code\n",
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02nh9mZZjXsl"
      },
      "source": [
        "##Authorize Google Colab & Read the positive and Negative Sentiment words from a Text File downloaded from  http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\n",
        "\n",
        "You need to download those files from above location and save into your google drive or local drive location frim where you will read the files in your program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GojeFsQve6GS",
        "outputId": "b82ef3c9-beaf-4ca1-cc52-c20fc2b83b8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "POSITIVE = 'positive'\n",
        "NEGATIVE = 'negative'\n",
        "BOTH = 'both'\n",
        "UNKNOWN = 'unknown'\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# Change below path to where you have saved your downloaded text files\n",
        "%cd /gdrive/My Drive/Colab Notebooks/opinion-lexicon-English/\n",
        "\n",
        "\n",
        "# Build the  positive word lists from files\n",
        "with open('positive-words.txt', 'rb') as f:\n",
        "         positive_set = set(nltk.word_tokenize(f.read().decode('ISO-8859-1')))\n",
        "\n",
        "# Build the  negative word lists from files\n",
        "with open('negative-words.txt', 'rb') as f:\n",
        "         negative_set = set(nltk.word_tokenize(f.read().decode('ISO-8859-1')))\n",
        "\n",
        "\n",
        "# Now we need to remove the headers(which are not part of the word list) from the word list files.\n",
        "# we can use set difference function to get rid of the header part\n",
        "positive_words = positive_set.difference(negative_set)\n",
        "negative_words = negative_set.difference(positive_set)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/Colab Notebooks/opinion-lexicon-English\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "print(f'positive_words: {len(positive_words)} — sample: {[w for w in itertools.islice(positive_words, 10)]}')\n",
        "print(f'negative_words: {len(negative_words)} — sample: {[w for w in itertools.islice(negative_words, 10)]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shs3TLu9MOUX",
        "outputId": "ed1563de-cf9d-46ad-efe6-fc1268fcea45"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive_words: 2004 — sample: ['well-intentioned', 'victory', 'righteousness', 'stainless', 'thrilling', 'compactly', 'valiantly', 'precisely', 'rightfully', 'stability']\n",
            "negative_words: 4784 — sample: ['overwhelmingly', 'overwhelmed', 'unforeseen', 'mockingly', 'overstatement', 'irrational', 'back-logged', 'leaking', 'hassled', 'languor']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFQCJXDuq31W"
      },
      "source": [
        "## Define the Parse_Sentiment Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWQSP1HtqEA8"
      },
      "source": [
        "def parse_sentiment(text):\n",
        "  sentences = [[word.lower() for word in nltk.word_tokenize(sentence)] for sentence in nltk.sent_tokenize(text)]\n",
        "  sents, words = count_sentiment(sentences)\n",
        "  print(words)\n",
        "  total = sum(words.values())\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def count_sentiment(sentences):\n",
        "  sents = Counter()\n",
        "  words = Counter()\n",
        "\n",
        "  for sentence in sentences:\n",
        "    sentiment = sentimentalize(sentence)\n",
        "    sents[sentiment] += 1\n",
        "    words[sentiment] += len(sentence)\n",
        "\n",
        "  return sents, words\n",
        "\n",
        "def sentimentalize(words):\n",
        "  positive_count = len(positive_words.intersection(words))\n",
        "  negative_count = len(negative_words.intersection(words))\n",
        "  if positive_count > 0 and negative_count > 0:\n",
        "    return BOTH\n",
        "  elif positive_count > 0:\n",
        "    return POSITIVE\n",
        "  elif negative_count > 0:\n",
        "    return NEGATIVE\n",
        "  else:\n",
        "    return UNKNOWN"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q5gdnfsrI_N"
      },
      "source": [
        "## Question: Parse the target Article directly from URL and calculate the overall sentiment score as being expressed in the news article"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install inscriptis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4yX3hCqQIvR",
        "outputId": "3fba5c78-c642-49b7-b28e-5895a173ccbb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: inscriptis in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: lxml>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from inscriptis) (4.9.4)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from inscriptis) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->inscriptis) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->inscriptis) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->inscriptis) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->inscriptis) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -IL 'https://www.nytimes.com/2017/01/26/arts/dance/rehearse-ice-feet-repeat-the-life-of-a-new-york-city-ballet-corps-dancer.html' | head -n 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH-HNOTpSk6O",
        "outputId": "60c45850-cb93-43da-d219-d4f84a4875f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0  276k    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "HTTP/2 200 \r\n",
            "x-b3-traceid: aeccc86240664716b8a868cc4e0535b4\r\n",
            "x-nyt-data-last-modified: Fri, 20 Sep 2024 21:44:56 GMT\r\n",
            "last-modified: Fri, 20 Sep 2024 21:44:56 GMT\r\n",
            "x-scoop-last-modified: 2017-12-22T02:31:06.552Z\r\n",
            "x-pagetype: vi-story\r\n",
            "x-xss-protection: 1; mode=block\r\n",
            "x-content-type-options: nosniff\r\n",
            "content-type: text/html; charset=utf-8\r\n",
            "x-envoy-upstream-service-time: 190\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEk56SbIrSgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b65e5a3-8b0a-4e74-bb15-d54a81f4e916"
      },
      "source": [
        "import urllib.request\n",
        "from inscriptis import get_text\n",
        "\n",
        "url = 'https://www.nytimes.com/2017/01/26/arts/dance/rehearse-ice-feet-repeat-the-life-of-a-new-york-city-ballet-corps-dancer.html'\n",
        "req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36'})\n",
        "html = urllib.request.urlopen(req).read().decode('utf-8')\n",
        "text = get_text(html)\n",
        "parse_sentiment(text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'both': 292, 'unknown': 192, 'positive': 146, 'negative': 95})\n"
          ]
        }
      ]
    }
  ]
}
